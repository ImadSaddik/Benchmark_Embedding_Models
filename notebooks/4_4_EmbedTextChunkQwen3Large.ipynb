{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2b335d",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fae5a",
   "metadata": {},
   "source": [
    "You will take the extracted text chunks and the questions that we generated in the previous steps and generate embeddings with an embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5146316",
   "metadata": {},
   "source": [
    "### Download the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82b838",
   "metadata": {},
   "source": [
    "Download the model in `GGUF` format from [Hugging Face](https://huggingface.co/Qwen/Qwen3-Embedding-8B-GGUF/tree/main). This requires more resources, so make sure you have enough RAM and VRAM.\n",
    "\n",
    "Download the following file:\n",
    "\n",
    "- `Qwen3-Embedding-8B-Q4_K_M.gguf`\n",
    "\n",
    "After that serve the model using [llama-server](https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md):\n",
    "\n",
    "```bash\n",
    "llama-server \\\n",
    "  --model ~/.cache/llama.cpp/Qwen3-Embedding-8B-Q4_K_M.gguf \\\n",
    "  --n-gpu-layers 999 \\\n",
    "  --ctx-size 6144 \\\n",
    "  --batch-size 6144 \\\n",
    "  --ubatch-size 6144 \\\n",
    "  --embedding \\\n",
    "  --pooling last \\\n",
    "  --port 36912\n",
    "```\n",
    "\n",
    "If you run out of memory, try to offload less layers to the GPU by reducing the value of `--n-gpu-layers`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2019aac5",
   "metadata": {},
   "source": [
    "### Prepare the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5268434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "LLAMA_SERVER_URL = \"http://localhost:36912\"\n",
    "\n",
    "\n",
    "def embed_text(text: str) -> list[float] | None:\n",
    "    embedding = None\n",
    "    response = requests.post(\n",
    "        url=f\"{LLAMA_SERVER_URL}/v1/embeddings\",\n",
    "        headers={\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        },\n",
    "        data=json.dumps({\"input\": text}),\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "\n",
    "    response_json = response.json()\n",
    "    embedding = response_json[\"data\"][0][\"embedding\"]\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b73af",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bedfd87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 43 text chunks.\n",
      "Loaded 377 questions.\n"
     ]
    }
   ],
   "source": [
    "prefix = \"rog_strix_gaming_notebook_pc_unscanned_file_chunks\"\n",
    "text_chunk_file_path = f\"../data/chunks/{prefix}.json\"\n",
    "questions_file_path = f\"../data/question_answer_pairs/{prefix}_qa_pairs.json\"\n",
    "\n",
    "with open(text_chunk_file_path, \"r\") as f:\n",
    "    text_chunks = json.load(f)\n",
    "\n",
    "with open(questions_file_path, \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(text_chunks)} text chunks.\")\n",
    "print(f\"Loaded {len(questions)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbae173",
   "metadata": {},
   "source": [
    "### Embed the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d40ae",
   "metadata": {},
   "source": [
    "Start by embedding the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b361d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:16<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model_name = \"qwen3-embedding-8b\"\n",
    "failed_text_chunks = []\n",
    "\n",
    "for chunk in tqdm(text_chunks, total=len(text_chunks)):\n",
    "    chunk_id = chunk[\"id\"]\n",
    "    text_chunk = chunk[\"text_chunk\"]\n",
    "\n",
    "    try:\n",
    "        text_chunk_embedding = embed_text(text_chunk)\n",
    "        chunk[\"embedding\"] = text_chunk_embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to embed chunk ID {chunk_id}: {e}\")\n",
    "        failed_text_chunks.append({\"id\": chunk_id, \"text_chunk\": text_chunk})\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3b19ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to embed 0 text chunks.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed to embed {len(failed_text_chunks)} text chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed0db5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 377/377 [00:11<00:00, 32.63it/s]\n"
     ]
    }
   ],
   "source": [
    "failed_questions = []\n",
    "\n",
    "for question_object in tqdm(questions, total=len(questions)):\n",
    "    chunk_id = question_object[\"chunk_id\"]\n",
    "    question = question_object[\"question\"]\n",
    "\n",
    "    try:\n",
    "        question_embedding = embed_text(question)\n",
    "        question_object[\"embedding\"] = question_embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to embed chunk ID {chunk_id}: {e}\")\n",
    "        failed_questions.append({\"chunk_id\": chunk_id, \"question\": question})\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f10cc43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to embed 0 questions.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed to embed {len(failed_questions)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ec42e",
   "metadata": {},
   "source": [
    "### Merge the lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aded67",
   "metadata": {},
   "source": [
    "You will generate a new list with the following structure:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"chunks\": [\n",
    "        {\n",
    "            \"id\": 0,\n",
    "            \"text_chunk\": \"\",\n",
    "            \"embeddings\": {\n",
    "                \"gemini-embedding-001\": [],\n",
    "            }\n",
    "        },\n",
    "        ...\n",
    "    ],\n",
    "    \"question_answer_pairs\": [\n",
    "        {\n",
    "            \"chunk_id\": 0,\n",
    "            \"question\": \"\",\n",
    "            \"embeddings\": {\n",
    "                \"gemini-embedding-001\": [],\n",
    "            }\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Additionally, before generating embeddings, the code checks if the merged list already exists on disk. If it does, the list is loaded instead of being recreated. This approach avoids unnecessary recomputation of embeddings and makes it easy to add new models to the embeddings dictionary without duplicating work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ad44585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/embeddings/rog_strix_gaming_notebook_pc_unscanned_file_chunks_embeddings.json already exists. Loading existing embeddings.\n",
      "Merged list has 43 chunks and 377 question-answer pairs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "embedding_directory = \"../data/embeddings\"\n",
    "if not os.path.exists(embedding_directory):\n",
    "    os.makedirs(embedding_directory)\n",
    "\n",
    "embedding_file_path = f\"{embedding_directory}/{prefix}_embeddings.json\"\n",
    "if os.path.exists(embedding_file_path):\n",
    "    print(f\"{embedding_file_path} already exists. Loading existing embeddings.\")\n",
    "    with open(embedding_file_path, \"r\") as f:\n",
    "        merged_list = json.load(f)\n",
    "\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        merged_list[\"chunks\"][i][\"embeddings\"][model_name] = chunk[\"embedding\"]\n",
    "\n",
    "    for i, question in enumerate(questions):\n",
    "        merged_list[\"question_answer_pairs\"][i][\"embeddings\"][model_name] = question[\n",
    "            \"embedding\"\n",
    "        ]\n",
    "\n",
    "else:\n",
    "    merged_list = {\"chunks\": [], \"question_answer_pairs\": []}\n",
    "    for chunk in text_chunks:\n",
    "        merged_list[\"chunks\"].append(\n",
    "            {\n",
    "                \"id\": chunk[\"id\"],\n",
    "                \"text_chunk\": chunk[\"text_chunk\"],\n",
    "                \"embeddings\": {model_name: chunk[\"embedding\"]},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    for question in questions:\n",
    "        merged_list[\"question_answer_pairs\"].append(\n",
    "            {\n",
    "                \"chunk_id\": question[\"chunk_id\"],\n",
    "                \"question\": question[\"question\"],\n",
    "                \"embeddings\": {model_name: question[\"embedding\"]},\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"Merged list has {len(merged_list['chunks'])} chunks and {len(merged_list['question_answer_pairs'])} question-answer pairs.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee3191",
   "metadata": {},
   "source": [
    "### Save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f94db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embedding_file_path, \"w\") as f:\n",
    "    json.dump(merged_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark_embedding_models_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
