{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ad8955",
   "metadata": {},
   "source": [
    "# Extract text from PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926225bc",
   "metadata": {},
   "source": [
    "Extracting text from PDFs is challenging because these files may be scanned, have complex layouts, or contain unstructured data such as images and tables. When building a dataset to benchmark embedding models, it is important to avoid noisy, poorly formatted, or merged text between sections.\n",
    "\n",
    "Libraries like [PyMuPDF](https://github.com/pymupdf/PyMuPDF), [PyPDF2](https://github.com/py-pdf/pypdf), and [pdfplumber](https://github.com/jsvine/pdfplumber) can extract text from simple PDFs. However, they are ineffective when dealing with unstructured data or scanned documents. Embedding models, unlike LLMs, cannot interpret the context or structure of a document. They simply embed whatever text they receive, regardless of its quality.\n",
    "\n",
    "Therefore, it is essential to ensure that the extracted text is clean and well-structured. Modern LLMs excel at reading images and understanding text, allowing us to leverage them to extract text from PDFs in a format that closely matches how a human would perceive the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc14d0",
   "metadata": {},
   "source": [
    "## PyMuPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c30bdb",
   "metadata": {},
   "source": [
    "### Normal PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb60f8",
   "metadata": {},
   "source": [
    "By a `normal PDF`, I mean a document that is not scanned and contains selectable text that can be extracted directly. Let's use the `pymupdf` library to extract text from each page of the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c8ed21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 26\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "\n",
    "pdf_path = \"../data/documents/the_state_of_ai_how_organizations_are_rewiring_to_capture_value_final.pdf\"\n",
    "pdf_document = pymupdf.open(pdf_path)\n",
    "print(f\"Number of pages: {pdf_document.page_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce37ce55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 50979 characters of text.\n"
     ]
    }
   ],
   "source": [
    "extracted_text = \"\"\n",
    "for page in pdf_document:\n",
    "    extracted_text += page.get_text()  # type: ignore\n",
    "print(f\"Extracted {len(extracted_text)} characters of text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7ef8c",
   "metadata": {},
   "source": [
    "We successfully extracted text from the PDF, but is it well-structured? Let's examine the first 2,000 characters to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7efddd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state of AI  \n",
      "March 2025\n",
      "Alex Singla  \n",
      "Alexander Sukharevsky  \n",
      "Lareina Yee  \n",
      "Michael Chui \n",
      "Bryce Hall\n",
      "How organizations are rewiring to capture value\n",
      "Organizations are beginning to create the \n",
      "structures and processes that lead to \n",
      "meaningful value from gen AI. While  \n",
      "still in early days, companies are  \n",
      "redesigning workflows, elevating  \n",
      "governance, and mitigating  \n",
      "more risks.   \n",
      "O\n",
      "rganizations are starting to make \n",
      "organizational changes designed to \n",
      "generate future value from gen AI, and \n",
      "large companies are leading the way. The \n",
      "latest McKinsey Global Survey on AI finds \n",
      "that organizations are beginning to take steps that drive \n",
      "bottom-line impact—for example, redesigning workflows as \n",
      "they deploy gen AI and putting senior leaders in critical roles, \n",
      "such as overseeing AI governance. The findings also show \n",
      "that organizations are working to mitigate a growing set of \n",
      "gen-AI-related risks and are hiring for new AI-related roles \n",
      "while they retrain employees to participate in AI deployment. \n",
      "Companies with at least $500 million in annual revenue \n",
      "are changing more quickly than smaller organizations. \n",
      "Overall, the use of AI—that is, gen AI as well as analytical \n",
      "AI—continues to build momentum: More than three-quarters \n",
      "of respondents now say that their organizations use AI in at \n",
      "least one business function. The use of gen AI in particular  \n",
      "is rapidly increasing. \n",
      "1\n",
      "The state of AI: How organizations are rewiring to capture value\n",
      "How companies are organizing \n",
      "their gen AI deployment—\n",
      "and who’s in charge    \n",
      " \n",
      "Our survey analyses show that a CEO’s oversight of AI governance—that is, the policies, \n",
      "processes, and technology necessary to develop and deploy AI systems responsibly—is one \n",
      "element most correlated with higher self-reported bottom-line impact from an organization’s \n",
      "gen AI use.1 That’s particularly true at larger companies, where CEO oversight is the element with \n",
      "the most impact on EBIT attributable to gen AI. Twenty-eight percent of respondents wh\n"
     ]
    }
   ],
   "source": [
    "print(extracted_text[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5361e",
   "metadata": {},
   "source": [
    "The answer is no, the text is not well-structured. This becomes clear on page 4, which contains a stacked bar chart.\n",
    "\n",
    "![example_of_unstructured_data_in_pdf](../images/example_of_unstructured_data_in_pdf.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bbd517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organizations are selectively centralizing elements of their  \n",
      "AI deployment \n",
      "The survey findings also shed light on how organizations are structuring their AI deployment \n",
      "efforts. Some essential elements for deploying AI tend to be fully or partially centralized  \n",
      "(Exhibit 1). For risk and compliance, as well as data governance, organizations often use a fully \n",
      "centralized model such as a center of excellence. For tech talent and adoption of AI solutions, \n",
      "on the other hand, respondents most often report using a hybrid or partially centralized model, \n",
      "with some resources handled centrally and others distributed across functions or business \n",
      "units—though respondents at organizations with less than $500 million in annual revenues  \n",
      "are more likely than others to report fully centralizing these elements. \n",
      "Exhibit 1\n",
      "Degree of centralization of AI deployment,¹ % of respondents\n",
      "McKinsey & Company\n",
      "¹Question was asked only of respondents whose organizations use AI in at least 1 function, n = 1,229. Figures were calculated after removing the share who said \n",
      "“don’t know/not applicable.”\n",
      "Source: McKinsey Global Survey on the state of AI, 1,491 participants at all levels of the organization, July 16–31, 2024\n",
      "Risk and data governance are two of the most centralized elements of \n",
      "deploying AI solutions, whereas tech talent is often hybrid.\n",
      "57\n",
      "46\n",
      "36\n",
      "35\n",
      "29\n",
      "23\n",
      "30\n",
      "39\n",
      "48\n",
      "44\n",
      "49\n",
      "54\n",
      "13\n",
      "15\n",
      "16\n",
      "21\n",
      "22\n",
      "23\n",
      "Fully centralized \n",
      "(eg, a hub or center \n",
      "of excellence is \n",
      "responsible across \n",
      "the organization)\n",
      "Hybrid (eg, some \n",
      "resources are\n",
      "primarily centralized\n",
      "and some are\n",
      "distributed across \n",
      "function)\n",
      "Fully distributed\n",
      "(eg, all resources \n",
      "live within the\n",
      "business functions)\n",
      "Risk and\n",
      "compliance\n",
      "Data\n",
      "governance \n",
      "for AI\n",
      "AI strategy\n",
      "Road map for \n",
      "AI-enhanced \n",
      "or AI-focused \n",
      "products\n",
      "Tech talent (eg, \n",
      "data engineers \n",
      "and machine \n",
      "learning\n",
      "engineers)\n",
      "Adoption\n",
      "of AI solutions \n",
      "(including \n",
      "changing pro-\n",
      "cesses, change \n",
      "management)\n",
      "CENTRALIZED\n",
      "“HUB” MODEL\n",
      "DECENTRALIZED\n",
      "“SPOKE” MODEL\n",
      "4\n",
      "The state of AI: How organizations are rewiring to capture value\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page_number = 4\n",
    "print(pdf_document[page_number].get_text())  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d9eaf",
   "metadata": {},
   "source": [
    "Other libraries such as `PyPDF2` and `pdfplumber` produce similar results. However, with LLMs that can interpret both text and images, we can extract and reconstruct the entire content of a PDF in a well-structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c284c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_document.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ce900f",
   "metadata": {},
   "source": [
    "### Scanned PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a40261",
   "metadata": {},
   "source": [
    "Even if a PDF is scanned, we can still determine the number of pages it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30d671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 19\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"../data/documents/rog_strix_gaming_notebook_pc_scanned_file.pdf\"\n",
    "pdf_document = pymupdf.open(pdf_path)\n",
    "print(f\"Number of pages: {pdf_document.page_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e5c3c",
   "metadata": {},
   "source": [
    "As you can see, the output shows 0 characters. This is because the PDF is scanned and contains images of text, rather than actual text that can be extracted directly. Let's explore how to handle this using LLMs. Check the next notebooks:\n",
    "\n",
    "**Proprietary models:**\n",
    "- [Gemini](./1_2_ExtractTextFomPDFsGemini.ipynb)\n",
    "\n",
    "**Open source models:**\n",
    "- [Granite docling 258M](./1_3_ExtractTextFomPDFsGraniteDocling258M.ipynb)\n",
    "- [Gemma3 12B](./1_4_ExtractTextFomPDFsGemma3.ipynb)\n",
    "- [Nanonets OCR2 3B](./1_5_ExtractTextFomPDFsNanonetsOCR2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b36efc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 0 characters of text.\n"
     ]
    }
   ],
   "source": [
    "extracted_text = \"\"\n",
    "for page in pdf_document:\n",
    "    extracted_text += page.get_text()  # type: ignore\n",
    "print(f\"Extracted {len(extracted_text)} characters of text.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark_embedding_models_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
