{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ad8955",
   "metadata": {},
   "source": [
    "# Extract text from PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926225bc",
   "metadata": {},
   "source": [
    "Extracting text from PDFs is challenging because these files may be scanned, have complex layouts, or contain unstructured data such as images and tables. When building a dataset to benchmark embedding models, it is important to avoid noisy, poorly formatted, or merged text between sections.\n",
    "\n",
    "Libraries like [PyMuPDF](https://github.com/pymupdf/PyMuPDF), [PyPDF2](https://github.com/py-pdf/pypdf), and [pdfplumber](https://github.com/jsvine/pdfplumber) can extract text from simple PDFs. However, they are ineffective when dealing with unstructured data or scanned documents. Embedding models, unlike LLMs, cannot interpret the context or structure of a document. They simply embed whatever text they receive, regardless of its quality.\n",
    "\n",
    "Therefore, it is essential to ensure that the extracted text is clean and well-structured. Modern LLMs excel at reading images and understanding text, allowing us to leverage them to extract text from PDFs in a format that closely matches how a human would perceive the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e655e",
   "metadata": {},
   "source": [
    "## LightOnOCR 1B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bea1dc",
   "metadata": {},
   "source": [
    "### Using llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa3170",
   "metadata": {},
   "source": [
    "Download the model in `GGUF` format from [Hugging Face](https://huggingface.co/ggml-org/LightOnOCR-1B-1025-GGUF). This requires more resources, so make sure you have enough RAM and VRAM.\n",
    "\n",
    "Download two files, one for the language model, and the other for the vision encoder (mmproj). The files are:\n",
    "\n",
    "- `LightOnOCR-1B-1025-Q8_0.gguf`\n",
    "- `mmproj-LightOnOCR-1B-1025-Q8_0.gguf`\n",
    "\n",
    "After that serve the model using [llama-server](https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md):\n",
    "\n",
    "```bash\n",
    "llama-server \\\n",
    "  --model ~/.cache/llama.cpp/LightOnOCR-1B-1025-Q8_0.gguf \\\n",
    "  --mmproj ~/.cache/llama.cpp/mmproj-LightOnOCR-1B-1025-Q8_0.gguf \\\n",
    "  --n-gpu-layers 99 \\\n",
    "  --ctx-size 8192 \\\n",
    "  --port 36912\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac36a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import mimetypes\n",
    "\n",
    "\n",
    "def encode_image_to_data_uri(image_path: str) -> str:\n",
    "    mime_type, _ = mimetypes.guess_type(image_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = \"application/octet-stream\"\n",
    "\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    return f\"data:{mime_type};base64,{encoded_string}\"\n",
    "\n",
    "\n",
    "path_to_image = \"../images/test_ocr.png\"\n",
    "image_data_uri = encode_image_to_data_uri(path_to_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a8c5b",
   "metadata": {},
   "source": [
    "On my RTX 4070, it took 6 seconds to process the image with LightOnOCR 1B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0fa3fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# Don't forget to start the llama.cpp server!\n",
    "LLAMA_SERVER_URL = \"http://localhost:36912\"\n",
    "\n",
    "\n",
    "def extract_text_from_images(image_data_uri: str) -> str:\n",
    "    url = f\"{LLAMA_SERVER_URL}/v1/chat/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_data_uri}},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    data = {\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 4096,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "        response.raise_for_status()\n",
    "        response_json = response.json()\n",
    "\n",
    "        assistant_message = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return assistant_message.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "model_response = extract_text_from_images(image_data_uri=image_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e28313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "while the kernel weights are structured as ($N_{\\rm slice}$, $N_{\\rm time}$). This precomputation significantly accelerates our calculations, which is essential since the longitudinal slices are at least partially degenerate with one another. Consequently, the fits require more steps and walkers to ensure proper convergence.\n",
      "\n",
      "To address this, we follow a similar approach to our sinusoidal fits using \\texttt{emcee}, but we increase the total number of steps to 100,000 and use 100 walkers. Nively, the fit would include $2 N_{\\rm slice} + 1$ parameters: $N_{\\rm slice}$ for the albedo values, $N_{\\rm slice}$ for the emission parameters, and one additional scatter parameter, $\\sigma$. However, since night-side slices do not contribute to the reflected light component, we exclude these albedo values from the fit. In any case, our choice of 100 walkers ensures a sufficient number of walkers per free parameter. Following \\citet{Coulombe2025} we set an upper prior limit of 3/2 on all albedo slices as a fully Lambertian sphere ($A_i = 1$) corresponds to a geometric albedo of $A_g = 2/3$. For thermal emission we impose a uniform prior between 0 and 500 ppm for each slice.\n",
      "\n",
      "We choose to fit our detrended lightcurves considering 4, 6 and 8 longitudinal slices ($N_{\\rm slice} = 4, 6, 8$). However, we show the results of the simplest 4 slice model. As in our previous fits, we conduct an initial run with 25,000 steps (25\\% of the total run) and use the maximum-probability parameters from this preliminary fit as the starting positions for the final 75,000-step run. We then discard the first 60\\% of the final run as burn-in.\n",
      "\n",
      "\\subsection{Planetary Effective Temperature}\n",
      "\n",
      "Phase curves are the only way to probe thermal emission from the day and nightside of an exoplanet and hence determine its global energy budget \\citep{Parmentier2018}. The wavelength range of NIRISS/SOSS covers a large portion of the emitted flux of WASP-121 b ($\\sim 50$--83\\%; see Figure~\\ref{fig:captured_flux}), enabling a precise and robust constraint of the planet's energy budget.\n",
      "\n",
      "We convert the fitted $F_p/F_*$ emission spectra to brightness temperature by wavelength,\n",
      "\\begin{equation}\n",
      "    T_{\\rm bright} = \\frac{hc}{k\\lambda} \\cdot \\left[ \\ln \\left( \\frac{2 hc^2}{\\lambda^5 B_{\\lambda,\\rm planet}} + 1 \\right) \\right]^{-1},\n",
      "\\end{equation}\n",
      "where the planet's thermal emission is\n",
      "\\begin{equation}\n",
      "    B_{\\lambda,\\rm planet} = \\frac{F_p/F_*}{(R_p/R_*)^2} \\cdot B_{\\lambda,\\rm star}.\n",
      "\\end{equation}\n",
      "\n",
      "There are many ways of converting brightness temperatures to effective temperature, including the Error-Weighted Mean (EWM), Power-Weighted mean (PWM) and with a Gaussian Process \\citep{Schwartz2015,Pass2019}. In this work, we elect to compute our effective temperature estimates with a novel method that is essentially a combination of the PWM and EWM. We create the effective temperature by using a simple Monte Carlo process. First, we perturb our $F_p/F_*$ emission spectra at each point in the orbit by a Gaussian based on the measurement uncertainty. Our new emission spectrum is then used to create an estimate of the brightness temperature spectrum. This process is repeated at each orbital phase. We then estimate the effective temperature, $T_{\\rm eff}$ for a given orbital phase as\n",
      "\\begin{equation}\n",
      "    T_{\\rm eff} = \\frac{\\sum_{i=1}^{N} w_i T_{\\rm bright,i}}{\\sum_{i=1}^{N} w_i},\n",
      "\\end{equation}\n",
      "where $w_i$ is the weight for the $i$-th wavelength given by the fraction of the planet's bolometric flux that falls within that wavelength bin scaled by the inverse variance of the measurement,\n",
      "\\begin{equation}\n",
      "    w_i = \\frac{\\int_{\\lambda_i}^{\\lambda_{i+1}} B(\\lambda_i, T_{\\rm est}) \\, d\\lambda}{\\int_{0}^{\\infty} B(\\lambda_i, T_{\\rm est}) \\, d\\lambda} \\cdot \\frac{1}{\\sigma_i^2},\n",
      "\\end{equation}\n",
      "with $T_{\\rm est}$ representing an estimated effective temperature at the orbital phase of interest. When computing\n",
      "\n",
      "\\begin{figure}[t]\n",
      "    \\centering\n",
      "    \\includegraphics[width=\\columnwidth]{captured_flux.pdf}\n",
      "    \\caption{Estimated captured flux of the planet assuming the planet radiates as a blackbody. The captured flux is calculated as the ratio of the integrated blackbody emission within the instrument's band pass to the total emission over all wavelengths, i.e., $\\gamma = \\int_{\\lambda_{\\rm min}}^{\\lambda_{\\rm max}} B(\\lambda, T) \\, d\\lambda / \\int_{0}^{\\infty} B(\\lambda, T) \\, d\\lambda$. The captured flux fraction is shown for NIRISS SOSS [0.6--2.85 $\\mu$m] (red line); Hubble WFC3 [1.12--1.64 $\\mu$m] (dashed green line); NIRSpec G39SH [2.7--5.15 $\\mu$m] (dash dotted blue line). The red-shaded region shows the temperature range on WASP-121 b based on our $T_{\\rm eff}$ estimates. Red dashed lines indicate the boundaries of the planet's temperature range within the NIRISS SOSS captured flux fraction. From this we estimate that these observations capture between 55\\% and 82\\% of the planet's bolometric flux, depending on orbital phase. Using the minimum temperature from the NAMELESS fit, this estimate decreases to 50\\%. In either case, the wavelength coverage of NIRISS exceeds that of any other instrument.}\n",
      "    \\label{fig:captured_flux}\n",
      "\\end{figure}\n"
     ]
    }
   ],
   "source": [
    "print(model_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark_embedding_models_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
