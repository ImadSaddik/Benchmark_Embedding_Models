{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2b335d",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fae5a",
   "metadata": {},
   "source": [
    "You will take the extracted text chunks and the questions that we generated in the previous steps and generate embeddings with an embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5146316",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f2e8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9c07d63a624a19b28f9b89992684b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fff2cf14c24347ba16c4bc69ec1775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e0c4d0a00e48efa46c5eb4e1cd2118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4a92fd62204859918bbd897376cfed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d4e0dc27cf4919b2eeaf05f879f00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300becc2db2149f696e78479f9d66965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0d0d07c11a49ef97399022f7c08e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b393888c75f4d32a22ed8d92814bc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ae461003e64145ba2cc363cb0f505c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca46c63798d495c9c38fcdfca62b0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model running on cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n",
    "model = model.to(device)\n",
    "print(f\"Model running on {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b73af",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bedfd87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 43 text chunks.\n",
      "Loaded 377 questions.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "prefix = \"rog_strix_gaming_notebook_pc_unscanned_file_chunks\"\n",
    "text_chunk_file_path = f\"../data/chunks/{prefix}.json\"\n",
    "questions_file_path = f\"../data/question_answer_pairs/{prefix}_qa_pairs.json\"\n",
    "\n",
    "with open(text_chunk_file_path, \"r\") as f:\n",
    "    text_chunks = json.load(f)\n",
    "\n",
    "with open(questions_file_path, \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(text_chunks)} text chunks.\")\n",
    "print(f\"Loaded {len(questions)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbae173",
   "metadata": {},
   "source": [
    "### Embed the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900f217",
   "metadata": {},
   "source": [
    "With `Qwen3-Embedding` models, you can pass a prompt to the `encode` method. This prompt helps the model generate more relevant embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "168fcd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Instruct: Given a web search query, retrieve relevant passages that answer the query\\nQuery:', 'document': ''}\n"
     ]
    }
   ],
   "source": [
    "print(model.prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d40ae",
   "metadata": {},
   "source": [
    "Start by embedding the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b361d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:04<00:00,  9.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sentence_transformers.SentenceTransformer import SentenceTransformer\n",
    "\n",
    "\n",
    "def embed_text(\n",
    "    model: SentenceTransformer, text: str, prompt_name: str\n",
    ") -> list[float] | None:\n",
    "    embedding = None\n",
    "    embedding = model.encode(sentences=text, prompt_name=prompt_name).tolist()\n",
    "    if not embedding:\n",
    "        raise ValueError(\"No embedding returned from the model.\")\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n",
    "model_name = \"qwen3-embedding-0.6b\"\n",
    "failed_text_chunks = []\n",
    "prompt_name = \"query\"\n",
    "\n",
    "for chunk in tqdm(text_chunks, total=len(text_chunks)):\n",
    "    chunk_id = chunk[\"id\"]\n",
    "    text_chunk = chunk[\"text_chunk\"]\n",
    "\n",
    "    try:\n",
    "        text_chunk_embedding = embed_text(model, text_chunk, prompt_name)\n",
    "        chunk[\"embedding\"] = text_chunk_embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to embed chunk ID {chunk_id}: {e}\")\n",
    "        failed_text_chunks.append({\"id\": chunk_id, \"text_chunk\": text_chunk})\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3b19ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to embed 0 text chunks.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed to embed {len(failed_text_chunks)} text chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed0db5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 377/377 [00:06<00:00, 61.78it/s]\n"
     ]
    }
   ],
   "source": [
    "failed_questions = []\n",
    "\n",
    "for question_object in tqdm(questions, total=len(questions)):\n",
    "    chunk_id = question_object[\"chunk_id\"]\n",
    "    question = question_object[\"question\"]\n",
    "\n",
    "    try:\n",
    "        question_embedding = embed_text(model, question, prompt_name)\n",
    "        question_object[\"embedding\"] = question_embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to embed chunk ID {chunk_id}: {e}\")\n",
    "        failed_questions.append({\"chunk_id\": chunk_id, \"question\": question})\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f10cc43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to embed 0 questions.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed to embed {len(failed_questions)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ec42e",
   "metadata": {},
   "source": [
    "### Merge the lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aded67",
   "metadata": {},
   "source": [
    "You will generate a new list with the following structure:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"chunks\": [\n",
    "        {\n",
    "            \"id\": 0,\n",
    "            \"text_chunk\": \"\",\n",
    "            \"embeddings\": {\n",
    "                \"gemini-embedding-001\": [],\n",
    "            }\n",
    "        },\n",
    "        ...\n",
    "    ],\n",
    "    \"question_answer_pairs\": [\n",
    "        {\n",
    "            \"chunk_id\": 0,\n",
    "            \"question\": \"\",\n",
    "            \"embeddings\": {\n",
    "                \"gemini-embedding-001\": [],\n",
    "            }\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Additionally, before generating embeddings, the code checks if the merged list already exists on disk. If it does, the list is loaded instead of being recreated. This approach avoids unnecessary recomputation of embeddings and makes it easy to add new models to the embeddings dictionary without duplicating work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ad44585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/embeddings/rog_strix_gaming_notebook_pc_unscanned_file_chunks_embeddings.json already exists. Loading existing embeddings.\n",
      "Merged list has 43 chunks and 377 question-answer pairs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "embedding_directory = \"../data/embeddings\"\n",
    "if not os.path.exists(embedding_directory):\n",
    "    os.makedirs(embedding_directory)\n",
    "\n",
    "embedding_file_path = f\"{embedding_directory}/{prefix}_embeddings.json\"\n",
    "if os.path.exists(embedding_file_path):\n",
    "    print(f\"{embedding_file_path} already exists. Loading existing embeddings.\")\n",
    "    with open(embedding_file_path, \"r\") as f:\n",
    "        merged_list = json.load(f)\n",
    "\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        merged_list[\"chunks\"][i][\"embeddings\"][model_name] = chunk[\"embedding\"]\n",
    "\n",
    "    for i, question in enumerate(questions):\n",
    "        merged_list[\"question_answer_pairs\"][i][\"embeddings\"][model_name] = question[\n",
    "            \"embedding\"\n",
    "        ]\n",
    "\n",
    "else:\n",
    "    merged_list = {\"chunks\": [], \"question_answer_pairs\": []}\n",
    "    for chunk in text_chunks:\n",
    "        merged_list[\"chunks\"].append(\n",
    "            {\n",
    "                \"id\": chunk[\"id\"],\n",
    "                \"text_chunk\": chunk[\"text_chunk\"],\n",
    "                \"embeddings\": {model_name: chunk[\"embedding\"]},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    for question in questions:\n",
    "        merged_list[\"question_answer_pairs\"].append(\n",
    "            {\n",
    "                \"chunk_id\": question[\"chunk_id\"],\n",
    "                \"question\": question[\"question\"],\n",
    "                \"embeddings\": {model_name: question[\"embedding\"]},\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"Merged list has {len(merged_list['chunks'])} chunks and {len(merged_list['question_answer_pairs'])} question-answer pairs.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee3191",
   "metadata": {},
   "source": [
    "### Save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f94db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embedding_file_path, \"w\") as f:\n",
    "    json.dump(merged_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark_embedding_models_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
