{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ad8955",
   "metadata": {},
   "source": [
    "# Extract text from PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926225bc",
   "metadata": {},
   "source": [
    "Extracting text from PDFs is challenging because these files may be scanned, have complex layouts, or contain unstructured data such as images and tables. When building a dataset to benchmark embedding models, it is important to avoid noisy, poorly formatted, or merged text between sections.\n",
    "\n",
    "Libraries like [PyMuPDF](https://github.com/pymupdf/PyMuPDF), [PyPDF2](https://github.com/py-pdf/pypdf), and [pdfplumber](https://github.com/jsvine/pdfplumber) can extract text from simple PDFs. However, they are ineffective when dealing with unstructured data or scanned documents. Embedding models, unlike LLMs, cannot interpret the context or structure of a document. They simply embed whatever text they receive, regardless of its quality.\n",
    "\n",
    "Therefore, it is essential to ensure that the extracted text is clean and well-structured. Modern LLMs excel at reading images and understanding text, allowing us to leverage them to extract text from PDFs in a format that closely matches how a human would perceive the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e655e",
   "metadata": {},
   "source": [
    "## Gemma3 12B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bea1dc",
   "metadata": {},
   "source": [
    "### Using llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa3170",
   "metadata": {},
   "source": [
    "Download the model in `GGUF` format from [Hugging Face](https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf). This requires more resources, so make sure you have enough RAM and VRAM.\n",
    "\n",
    "Download two files, one for the language model, and the other for the vision encoder (mmproj). The files are:\n",
    "\n",
    "- `gemma-3-12b-it-q4_0.gguf`\n",
    "- `mmproj-model-f16-12B.gguf`\n",
    "\n",
    "After that serve the model using [llama-server](https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md):\n",
    "\n",
    "```bash\n",
    "llama-server \\\n",
    "  --model ~/.cache/llama.cpp/gemma-3-12b-it-q4_0.gguf \\\n",
    "  --mmproj ~/.cache/llama.cpp/mmproj-model-f16-12B.gguf \\\n",
    "  --n-gpu-layers 20 \\\n",
    "  --ctx-size 4096 \\\n",
    "  --port 36912\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac36a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import mimetypes\n",
    "\n",
    "\n",
    "def encode_image_to_data_uri(image_path: str) -> str:\n",
    "    mime_type, _ = mimetypes.guess_type(image_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = \"application/octet-stream\"\n",
    "\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    return f\"data:{mime_type};base64,{encoded_string}\"\n",
    "\n",
    "\n",
    "path_to_image = \"../images/test_ocr.png\"\n",
    "image_data_uri = encode_image_to_data_uri(path_to_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a8c5b",
   "metadata": {},
   "source": [
    "On my RTX 4070, it took 6 minutes to process the image with Gemma3 12B. This is slow but let's see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa3fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# Don't forget to start the llama.cpp server!\n",
    "LLAMA_SERVER_URL = \"http://localhost:36912\"\n",
    "\n",
    "\n",
    "def extract_text_from_images(model_name: str, prompt: str, image_data_uri: str) -> str:\n",
    "    url = f\"{LLAMA_SERVER_URL}/v1/chat/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_data_uri}},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    data = {\n",
    "        \"messages\": messages,\n",
    "        \"model\": model_name,\n",
    "        \"max_tokens\": 4096,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "        response.raise_for_status()\n",
    "        response_json = response.json()\n",
    "\n",
    "        assistant_message = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return assistant_message.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "model_to_use = \"gemma-3-12b-it\"\n",
    "system_prompt = \"\"\"You are an expert AI assistant, you are tasked with extracting the entire text from any PDF document. The document can be simple, complex, or even scanned, this shouldn't matter to you.\n",
    "\n",
    "You will be given the entire PDF as input. Start examining the document page by page, when you come across text, extract it as is don't convert it into another format like HTML or Markdown. If you come across images, replace them with a very detailed description of the image while taking into consideration the context around it.\n",
    "\n",
    "When you come across tables, describe them too like the image. The description should be very detailed and in a way that someone will understand the table without seeing it.\n",
    "\n",
    "Make sure to keep the structure of the document, if there are sections, subsections, bullet points, or numbered lists, make sure to keep them as is. If there are any headers, footers, page numbers, remove them.\n",
    "\n",
    "The final output should be a clean, well-structured text that represents the content of the entire PDF document as closely as possible to how a human would see it with their eyes when reading the document. Don't say anything else, just output the text you extracted from the PDF.\n",
    "\n",
    "Here is the PDF:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "gemma_response = extract_text_from_images(\n",
    "    model_name=model_to_use, prompt=system_prompt, image_data_uri=image_data_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9e28313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENERGY BUDGET OF WASP-121 b FROM JWST/NIRISS PHASE CURVE 9\n",
      "\n",
      "This preconputation significantly affects our calculations, which is essential since the longitudinal sides are at least partially degenerate with one another. Consequently, the fits require more steps and walkers to ensure proper convergence.\n",
      "\n",
      "To address this, we follow a similar approach to our sinusoidal fits using emcee, but we increase the total number of walkers to 1000 and use 100 walkers. Nively, the fit would include 2N<sub>λ</sub> + 1 parameters. N<sub>λ</sub> refers to the number of walkers for the emission parameters and one additional scatter parameter. However, since night-side slices do not contribute to the reflected light component, we exclude these absorbed values from the fit. In any case, our choice of 100 walkers ensures a sufficient number of walkers per free parameter. Following Coulombe et al. (2023), we set an upper prior limit of 3/2 on all ellipsoidal shapes as a spherical shape (A<sub>1</sub> = 1), corresponds to a geometric albedo of A<sub>1</sub> = 2/3. For thermal emission we impose a uniform prior between 0 and 500 ppm for each slice.\n",
      "\n",
      "We chose to fit our detrended light curves considering 4, 6, and 8 longitudinal slices (N<sub>λ</sub> = 4, 6, 8). However, we show the results of the simplest slice model. As in our previous fits, we consider an ideal radius with 25.00 ppm (28% of the total run) and use the uniform probability parameterization with a minimum uniform probability starting post the final 75 steps.\n",
      "\n",
      "The fits for 6% of the 7500 μm run.\n",
      "\n",
      "In Figure 2, we show the best-fit parameters with 1σ uncertainties for each model. The 4-slice model is significantly worse than the 6-slice model, with a reduced chi-squared of χ<sup>2</sup><sub>red</sub> = 1.50 versus 1.05. The 8-slice model is only marginally better than the 6-slice model, with a reduced chi-squared of χ<sup>2</sup><sub>red</sub> = 1.02. Thus, we favor the 6-slice model.\n",
      "\n",
      "The best-fit geometric albedo of A<sub>1</sub> = 0.26 ± 0.04. The best-fit temperature of the dayside is T<sub>d</sub> = 1702 ± 21 K, and the best-fit temperature of the nightside is T<sub>n</sub> = 984 ± 36 K. The dayside-nightside temperature difference is 718 K, which is consistent with previous estimates. The captured flux fraction for NIRISS SOSS (0.6–1.1 μm) is 0.82%, and the captured flux fraction for NIRISS G395H (2.7–5.6 μm) is 0.85%.\n",
      "\n",
      "Figure 2. Estimated captured flux of the planet assuming the planet radiates as a blackbody. The captured flux is calculated as the ratio of the integrated blackbody emission within the instrument’s bandpass to the total emission at wavelengths, f<sub>c</sub> = ∫ B(λ, T) / ∫ B(λ, T) dλ. The captured flux fractions shown for NIRISS SOSS (0.6–1.1 μm) (red line) and NIRISS G395H (2.7–5.6 μm) (dashed green line). The dotted blue line represents the temperature range where NIRISS G395H is sensitive. Red dashed lines indicate the boundaries of our T<sub>d</sub> estimates. The star’s contribution is shown as a solid black line.\n",
      "\n",
      "From this, we estimate that NIRISS SOSS captures he approximately 55% and 82% of the planet’s total flux, depending on the wavelength. Using the estimated temperature and albedo, we find that the planet’s captured flux fraction is 56% at 0.8 μm and 84% at 2.8 μm.\n",
      "\n",
      "We have also investigated the effect of allowing the planet’s albedo to vary with wavelength. We find that the best-fit wavelength-dependent albedo is A<sub>λ</sub> = 0.17 ± 0.04 at 0.8 μm and A<sub>λ</sub> = 0.33 ± 0.05 at 2.8 μm. However, the fit is only marginally better than the wavelength-independent albedo model, with a reduced chi-squared of χ<sup>2</sup><sub>red</sub> = 1.03 versus 1.05. Thus, we do not favor the wavelength-dependent albedo model.\n",
      "\n",
      "We find that the dayside temperature is consistent with the expected temperature from the energy budget, which is T<sub>d</sub> ≈ 1660 K. The nightside temperature is also consistent with the expected temperature, which is T<sub>n</sub> ≈ 960 K.\n",
      "\n",
      "We have also investigated the effect of allowing the planet’s radius to vary. We find that the best-fit radius is R<sub>p</sub> = 1.31 ± 0.07 R<sub>J</sub>, which is consistent with previous estimates. However, the fit is only marginally better than the fixed radius model, with a reduced chi-squared of χ<sup>2</sup><sub>red</sub> = 1.04 versus 1.05. Thus, we do not favor the varying radius model.\n",
      "\n",
      "Figure 2 shows the estimated captured flux of the planet assuming the planet radiates as a blackbody. The captured flux is calculated as the ratio of the integrated blackbody emission within the instrument’s bandpass to the total emission at wavelengths, f<sub>c</sub> = ∫ B(λ, T) / ∫ B(λ, T) dλ. The captured flux fractions shown for NIRISS SOSS (0.6–1.1 μm) (red line) and NIRISS G395H (2.7–5.6 μm) (dashed green line). The dotted blue line represents the temperature range where NIRISS G395H is sensitive. Red dashed lines indicate the boundaries of our T<sub>d</sub> estimates. The star’s contribution is shown as a solid black line.\n",
      "\n",
      "From this, we estimate that NIRISS SOSS captures he approximately 55% and 82% of the planet’s total flux, depending on the wavelength. Using the estimated temperature and albedo, we find that the planet’s captured flux fraction is 56% at 0.8 μm and 84% at 2.8 μm.\n",
      "\n",
      "We have also investigated the effect of allowing the planet’s albedo to vary with wavelength. We find that the best-fit wavelength-dependent albedo is A<sub>λ</sub> = 0.17 ± 0.04 at 0.8 μm and A<sub>λ</sub> = 0.33 ± 0.05 at 2.8 μm. However, the fit is only marginally better than the wavelength-independent albedo model, with a reduced chi-squared of χ<sup>2</sup><sub>red</sub> = 1.03 versus 1.05. Thus, we do not favor the wavelength-dependent albedo model.\n",
      "\n",
      "We find that the dayside temperature is consistent with the expected temperature from the energy budget, which is T<sub>d</sub> ≈ 1660 K. The nightside temperature is also consistent with the expected temperature, which is T<sub>n</sub> ≈ 960 K.\n",
      "\n",
      "We have also investigated the effect of allowing the planet’s radius to vary. We find that the best-fit radius is R<sub>p</sub> = 1.31 ± 0.07 R<sub>J</sub>, which is consistent with previous estimates. However, the fit is only marginally better than the fixed radius model, with a reduced chi-squared of χ<sup>2</sup><sub>red</sub> = 1.04 versus 1.05. Thus, we do not favor the varying radius model.\n",
      "\n",
      "The figure shows a graph with the x-axis labeled \"Temperature [100 K]\" ranging from 800 to 3800. The y-axis is labeled \"Captured flux fraction\". A black solid line represents the star's contribution, starting at approximately 0.1 and decreasing to 0.02 at 1600 K, then remaining relatively constant at 0.02 until 3800 K. A red line represents the NIRISS SOSS (0.6–1.1 μm) captured flux, which peaks at approximately 0.82% at around 1700 K and then decreasing to 0.05% at 3800 K. A dashed green line represents the NIRISS G395H (2.7–5.6 μm) captured flux, which peaks at approximately 0.85% at around 1700 K and then decreasing to 0.03% at 3800 K. There are two red dashed lines indicating the boundaries of the T<sub>d</sub> estimates. A dotted blue line represents the temperature range where NIRISS G395H is sensitive.\n"
     ]
    }
   ],
   "source": [
    "print(gemma_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400719ab",
   "metadata": {},
   "source": [
    "Let's compare the first paragraph between the following models:\n",
    "\n",
    "- `Gemma3 12B`\n",
    "- `Granite docling 258M`\n",
    "- `Gemini 2.5 Pro`\n",
    "\n",
    "**Gemini 2.5 Pro:** Perfectly extracted text.\n",
    "\n",
    "ENERGY BUDGET OF WASP-121 b FROM JWST/NIRISS PHASE CURVE\n",
    "\n",
    "while the kernel weights are structured as ($N_{albedo}$, $N_{time}$). This precomputation significantly accelerates our calculations, which is essential since the longitudinal slices are at least partially degenerate with one another. Consequently, the fits require more steps and walkers to ensure proper convergence.\n",
    "\n",
    "---\n",
    "\n",
    "**Gemma3 12B:** Missing the first sentence, and some words are incorrect.\n",
    "\n",
    "ENERGY BUDGET OF WASP-121 b FROM JWST/NIRISS PHASE CURVE 9\n",
    "\n",
    "This ~~preconputation~~ significantly ~~affects~~ our calculations, which is essential since the longitudinal ~~sides~~ are at least partially degenerate with one another. Consequently, the fits require more steps and walkers to ensure proper convergence.\n",
    "\n",
    "---\n",
    "\n",
    "**Granite docling 258M:** Perfectly extracted text like the big Gemini 2.5 Pro model.\n",
    "\n",
    "ENERGY BUDGET OF WASP-121 b FROM JWST/NIRISS PHASE CURVE 9\n",
    "\n",
    "while the kernel weights are structured as ( N$_{slice}$ , N$_{time}$ ). This precomputation significantly accelerates our calculations, which is essential since the longitudinal slices are at least partially degenerate with one another. Consequently, the fits require more steps and walkers to ensure proper convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90831fa",
   "metadata": {},
   "source": [
    "`Granite docling 258M` performed surprisingly well, matching the output of `Gemini 2.5 Pro`, while `Gemma3 12B` missed some words and made mistakes. `IBM` trained `Granite docling 258M` to be extremely good at document understanding, that's why it performed so well. `Gemma3 12B` is a general-purpose model, so it is not as good at this specific task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark_embedding_models_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
