{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2b335d",
   "metadata": {},
   "source": [
    "# Translate text to other languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fae5a",
   "metadata": {},
   "source": [
    "You will take the text chunks and questions that you have created in the previous notebook and translate them to other languages using an open weights model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e508c36",
   "metadata": {},
   "source": [
    "Define the target language you want to translate to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5a47aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_language = \"arabic\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5146316",
   "metadata": {},
   "source": [
    "### Download the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82b838",
   "metadata": {},
   "source": [
    "Download the model in `GGUF` format from [Hugging Face](https://huggingface.co/bartowski/aya-expanse-32b-GGUF/tree/main). This requires more resources, so make sure you have enough RAM and VRAM.\n",
    "\n",
    "Download the following file:\n",
    "\n",
    "- `aya-expanse-32b-Q4_K_M.gguf`\n",
    "\n",
    "After that serve the model using [llama-server](https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md):\n",
    "\n",
    "```bash\n",
    "llama-server \\\n",
    "  --model ~/.cache/llama.cpp/aya-expanse-32b-Q4_K_M.gguf \\\n",
    "  --n-gpu-layers 5 \\\n",
    "  --ctx-size 4096 \\\n",
    "  --batch-size 2048 \\\n",
    "  --ubatch-size 2048 \\\n",
    "  --port 36912\n",
    "```\n",
    "\n",
    "If you run out of memory, try to offload less layers to the GPU by reducing the value of `--n-gpu-layers`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff9510",
   "metadata": {},
   "source": [
    "### Create the system prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833bdfb",
   "metadata": {},
   "source": [
    "Create a system prompt to teach the LLM how to perform the translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34679434",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "## Task description\n",
    "\n",
    "You are a world-class translator. Your task is to translate a given text into a specified target language. Don't do a literal word-for-word translation; instead, ensure that the translated text captures the original meaning, tone, and context.\n",
    "\n",
    "### Important instructions:\n",
    "\n",
    "- Don't add or remove any information from the original text.\n",
    "- Maintain the original tone and style of the text.\n",
    "- Return only the translated text without any additional commentary or explanations.\n",
    "\n",
    "Here is the text, please translate it into {target_language}:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dc62b6",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05b39776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14 text chunks.\n",
      "Loaded 135 questions.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "prefix = \"glide_the_state_of_ai_in_operations_2025_report_chunks\"\n",
    "text_chunk_file_path = f\"../data/chunks/{prefix}.json\"\n",
    "questions_file_path = f\"../data/question_answer_pairs/{prefix}_qa_pairs.json\"\n",
    "\n",
    "with open(text_chunk_file_path, \"r\") as f:\n",
    "    text_chunks = json.load(f)\n",
    "\n",
    "with open(questions_file_path, \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(text_chunks)} text chunks.\")\n",
    "print(f\"Loaded {len(questions)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2019aac5",
   "metadata": {},
   "source": [
    "### Prepare the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5268434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Don't forget to start the llama.cpp server!\n",
    "LLAMA_SERVER_URL = \"http://localhost:36912\"\n",
    "\n",
    "\n",
    "def translate_text(system_prompt: str, text: str, max_tokens: int = 4096) -> str | None:\n",
    "    url = f\"{LLAMA_SERVER_URL}/v1/chat/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "\n",
    "    data = {\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url=url, headers=headers, data=json.dumps(data))\n",
    "        response.raise_for_status()\n",
    "        response_json = response.json()\n",
    "\n",
    "        translated_text = response_json[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        return translated_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbae173",
   "metadata": {},
   "source": [
    "### Translate the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d2bbc",
   "metadata": {},
   "source": [
    "Translate the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06524718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for chunk in tqdm(text_chunks, total=len(text_chunks)):\n",
    "    text = chunk[\"text_chunk\"]\n",
    "    response = translate_text(system_prompt, text)\n",
    "    if response is None:\n",
    "        raise Exception(\"Translation failed.\")\n",
    "\n",
    "    chunk[\"text_chunk\"] = response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb7119",
   "metadata": {},
   "source": [
    "This cell displays the Arabic text with proper right-to-left alignment to make it more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20750dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p dir=\"rtl\" style=\"text-align: right; font-family: Arial, sans-serif; line-height: 1.8;\">معلومات حقوق النشر\r<br>لا يجوز استنساخ أي جزء من هذا الدليل، بما في ذلك المنتجات والبرامج الموصوفة فيه، أو نقله، أو تدوينه، أو تخزينه في نظام استرجاع، أو ترجمته إلى أي لغة بأي شكل من الأشكال أو بأي وسيلة، باستثناء الوثائق التي يحتفظ بها المشتري لأغراض النسخ الاحتياطي، دون الحصول على إذن خطي صريح من شركة ASUSTeK COMPUTER INC. (\"ASUS\").\r<br>\r<br>توفر شركة ASUS هذا الدليل \"كما هو\" دون أي ضمان من أي نوع، سواء كان صريحًا أو ضمنيًا، بما في ذلك على سبيل المثال لا الحصر، الضمانات أو الشروط الضمنية القابلة للتسويق أو الملاءمة لغرض معين. لن تكون شركة ASUS، أو مديريتها، أو موظفيها، أو وكلائها مسؤولين عن أي أضرار غير مباشرة، أو خاصة، أو عرضية، أو تبعية (بما في ذلك الأضرار الناجمة عن فقدان الأرباح، أو فقدان الأعمال، أو فقدان الاستخدام أو البيانات، أو تعطل الأعمال وما شابه ذلك)، حتى لو تم إخطار شركة ASUS باحتمال حدوث مثل هذه الأضرار الناجمة عن أي عيب أو خطأ في هذا الدليل أو المنتج.\r<br>\r<br>قد تكون أسماء المنتجات والشركات المذكورة في هذا الدليل علامات تجارية أو حقوق نشر مسجلة أو غير مسجلة للشركات المعنية، ويتم استخدامها فقط للتعريف أو الشرح ولمصلحة المالكين، دون نية الانتهاك.\r<br>\r<br>المواصفات والمعلومات الواردة في هذا الدليل مخصصة للاستخدام الإعلامي فقط، وهي خاضعة للتغيير في أي وقت دون إشعار، ولا يجب اعتبارها التزامًا من قبل شركة ASUS. لا تتحمل شركة ASUS أي مسؤولية أو تبعة عن أي أخطاء أو عدم دقة قد تظهر في هذا الدليل، بما في ذلك المنتجات والبرامج الموصوفة فيه.\r<br>\r<br>حقوق النشر © 2024 ASUSTeK COMPUTER INC. جميع الحقوق محفوظة.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "text = text_chunks[1][\"text_chunk\"]\n",
    "text_with_breaks = text.replace(\"\\n\", \"<br>\")\n",
    "display(\n",
    "    HTML(\n",
    "        f'<p dir=\"rtl\" style=\"text-align: right; font-family: Arial, sans-serif; line-height: 1.8;\">{text_with_breaks}</p>'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036d0e5",
   "metadata": {},
   "source": [
    "Now, translate the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b37339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for question_object in tqdm(questions, total=len(questions)):\n",
    "    question_text = question_object[\"question\"]\n",
    "    response = translate_text(system_prompt, question_text)\n",
    "    if response is None:\n",
    "        raise Exception(\"Translation failed.\")\n",
    "\n",
    "    question_object[\"question\"] = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d3c924",
   "metadata": {},
   "source": [
    "### Save the translated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53983630",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_text_chunk_file_path = f\"../data/chunks/{prefix}_{target_language}.json\"\n",
    "translated_questions_file_path = (\n",
    "    f\"../data/question_answer_pairs/{prefix}_qa_pairs_{target_language}.json\"\n",
    ")\n",
    "\n",
    "with open(translated_text_chunk_file_path, \"w\") as f:\n",
    "    json.dump(text_chunks, f, indent=2)\n",
    "\n",
    "with open(translated_questions_file_path, \"w\") as f:\n",
    "    json.dump(questions, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark_embedding_models_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
