{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2b335d",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fae5a",
   "metadata": {},
   "source": [
    "You will take the extracted text chunks and the questions that we generated in the previous steps and generate embeddings with an embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b73af",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bedfd87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 43 text chunks.\n",
      "Loaded 377 questions.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "prefix = \"rog_strix_gaming_notebook_pc_unscanned_file_chunks\"\n",
    "text_chunk_file_path = f\"../data/chunks/{prefix}.json\"\n",
    "questions_file_path = f\"../data/question_answer_pairs/{prefix}_qa_pairs.json\"\n",
    "\n",
    "with open(text_chunk_file_path, \"r\") as f:\n",
    "    text_chunks = json.load(f)\n",
    "\n",
    "with open(questions_file_path, \"r\") as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(text_chunks)} text chunks.\")\n",
    "print(f\"Loaded {len(questions)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbae173",
   "metadata": {},
   "source": [
    "### Embed the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f22a8c",
   "metadata": {},
   "source": [
    "The OpenAI `text-embedding-3-large` model can be accessed via the OpenAI API. Here are some details about the model:\n",
    "\n",
    "| Model Name              | Price per 1M Tokens | Price per 1M Tokens (Batch) | Max input tokens |\n",
    "|-------------------------|----------------------------|------------------------------|-------------------|\n",
    "| text-embedding-3-large  | $0.13                      | $0.065                        | 8192              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d40ae",
   "metadata": {},
   "source": [
    "Start by embedding the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b361d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:15<00:00,  2.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def embed_text(client: OpenAI, text: str, model_name: str) -> list[float] | None:\n",
    "    embedding = None\n",
    "    response = client.embeddings.create(input=text, model=model_name)\n",
    "    embedding = response.data[0].embedding\n",
    "    if not embedding:\n",
    "        raise ValueError(\"No embedding returned from the model.\")\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n",
    "model_name = \"text-embedding-3-large\"\n",
    "client = OpenAI()\n",
    "failed_text_chunks = []\n",
    "\n",
    "for chunk in tqdm(text_chunks, total=len(text_chunks)):\n",
    "    chunk_id = chunk[\"id\"]\n",
    "    text_chunk = chunk[\"text_chunk\"]\n",
    "\n",
    "    try:\n",
    "        text_chunk_embedding = embed_text(client, text_chunk, model_name)\n",
    "        chunk[\"embedding\"] = text_chunk_embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to embed chunk ID {chunk_id}: {e}\")\n",
    "        failed_text_chunks.append({\"id\": chunk_id, \"text_chunk\": text_chunk})\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b19ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to embed 0 text chunks.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed to embed {len(failed_text_chunks)} text chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ed0db5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 377/377 [02:14<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "failed_questions = []\n",
    "\n",
    "for question_object in tqdm(questions, total=len(questions)):\n",
    "    chunk_id = question_object[\"chunk_id\"]\n",
    "    question = question_object[\"question\"]\n",
    "\n",
    "    try:\n",
    "        question_embedding = embed_text(client, question, model_name)\n",
    "        question_object[\"embedding\"] = question_embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to embed chunk ID {chunk_id}: {e}\")\n",
    "        failed_questions.append({\"chunk_id\": chunk_id, \"question\": question})\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f10cc43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to embed 0 questions.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Failed to embed {len(failed_questions)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a9bced",
   "metadata": {},
   "source": [
    "The requests I made costed me practically nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75c18478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens embedded: 23188\n",
      "Total cost: $0.003014\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def num_tokens_from_string(text: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "total_tokens = 0\n",
    "for chunk in text_chunks:\n",
    "    text_chunk = chunk[\"text_chunk\"]\n",
    "    total_tokens += num_tokens_from_string(text_chunk, \"cl100k_base\")\n",
    "\n",
    "for question_object in questions:\n",
    "    question = question_object[\"question\"]\n",
    "    total_tokens += num_tokens_from_string(question, \"cl100k_base\")\n",
    "\n",
    "price_per_million_tokens_usd = 0.13\n",
    "total_cost_usd = (total_tokens / 1_000_000) * price_per_million_tokens_usd\n",
    "print(f\"Total tokens embedded: {total_tokens}\")\n",
    "print(f\"Total cost: ${total_cost_usd:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ec42e",
   "metadata": {},
   "source": [
    "### Merge the lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aded67",
   "metadata": {},
   "source": [
    "You will generate a new list with the following structure:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"chunks\": [\n",
    "        {\n",
    "            \"id\": 0,\n",
    "            \"text_chunk\": \"\",\n",
    "            \"embeddings\": {\n",
    "                \"gemini-embedding-001\": [],\n",
    "            }\n",
    "        },\n",
    "        ...\n",
    "    ],\n",
    "    \"question_answer_pairs\": [\n",
    "        {\n",
    "            \"chunk_id\": 0,\n",
    "            \"question\": \"\",\n",
    "            \"embeddings\": {\n",
    "                \"gemini-embedding-001\": [],\n",
    "            }\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Additionally, before generating embeddings, the code checks if the merged list already exists on disk. If it does, the list is loaded instead of being recreated. This approach avoids unnecessary recomputation of embeddings and makes it easy to add new models to the embeddings dictionary without duplicating work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ad44585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/embeddings/rog_strix_gaming_notebook_pc_unscanned_file_chunks_embeddings.json already exists. Loading existing embeddings.\n",
      "Merged list has 43 chunks and 377 question-answer pairs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "embedding_directory = \"../data/embeddings\"\n",
    "if not os.path.exists(embedding_directory):\n",
    "    os.makedirs(embedding_directory)\n",
    "\n",
    "embedding_file_path = f\"{embedding_directory}/{prefix}_embeddings.json\"\n",
    "if os.path.exists(embedding_file_path):\n",
    "    print(f\"{embedding_file_path} already exists. Loading existing embeddings.\")\n",
    "    with open(embedding_file_path, \"r\") as f:\n",
    "        merged_list = json.load(f)\n",
    "\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        merged_list[\"chunks\"][i][\"embeddings\"][model_name] = chunk[\"embedding\"]\n",
    "\n",
    "    for i, question in enumerate(questions):\n",
    "        merged_list[\"question_answer_pairs\"][i][\"embeddings\"][model_name] = question[\n",
    "            \"embedding\"\n",
    "        ]\n",
    "\n",
    "else:\n",
    "    merged_list = {\"chunks\": [], \"question_answer_pairs\": []}\n",
    "    for chunk in text_chunks:\n",
    "        merged_list[\"chunks\"].append(\n",
    "            {\n",
    "                \"id\": chunk[\"id\"],\n",
    "                \"text_chunk\": chunk[\"text_chunk\"],\n",
    "                \"embeddings\": {model_name: chunk[\"embedding\"]},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    for question in questions:\n",
    "        merged_list[\"question_answer_pairs\"].append(\n",
    "            {\n",
    "                \"chunk_id\": question[\"chunk_id\"],\n",
    "                \"question\": question[\"question\"],\n",
    "                \"embeddings\": {model_name: question[\"embedding\"]},\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\n",
    "    f\"Merged list has {len(merged_list['chunks'])} chunks and {len(merged_list['question_answer_pairs'])} question-answer pairs.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee3191",
   "metadata": {},
   "source": [
    "### Save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02f94db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embedding_file_path, \"w\") as f:\n",
    "    json.dump(merged_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark_embedding_models_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
