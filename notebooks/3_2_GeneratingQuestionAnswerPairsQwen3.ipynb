{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61ac29b3",
   "metadata": {},
   "source": [
    "## Generate Question-Answer Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b161d40b",
   "metadata": {},
   "source": [
    "You will take the chunks of text from the previous step and generate question-answer pairs based on the content of each chunk.\n",
    "\n",
    "Each chunk can be associated to one or more questions. The questions will be generated by an LLM, but you will need to make sure that the questions are relevant to the content of the chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3040fd4",
   "metadata": {},
   "source": [
    "## Qwen3-30B-A3B-Instruct-2507"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a472dacf",
   "metadata": {},
   "source": [
    "Download the model in `GGUF` format from [Hugging Face](https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF/tree/main). This requires more resources, so make sure you have enough RAM and VRAM.\n",
    "\n",
    "Download one of the quantized models, I chose the following:\n",
    "\n",
    "- `Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf`\n",
    "\n",
    "After that serve the model using [llama-server](https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md):\n",
    "\n",
    "\n",
    "```bash\n",
    "llama-server \\\n",
    "  --model ~/.cache/llama.cpp/Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf \\\n",
    "  --n-gpu-layers 18 \\\n",
    "  --ctx-size 4096 \\\n",
    "  --port 36912\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f078a",
   "metadata": {},
   "source": [
    "### Create the system prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0c788",
   "metadata": {},
   "source": [
    "Start by creating the system prompt to teach the LLM how to generate questions given a chunk of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b86f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "## Task description\n",
    "\n",
    "You are an expert at generating relevant questions given some chunk of text. Your task is to read that text and create one or more questions that accurately reflect the key points, or information contained within that chunk.\n",
    "\n",
    "This task is important for creating question-answer pairs that will be used to benchmark embedding models.\n",
    "\n",
    "## Output format\n",
    "\n",
    "You will output a JSON array of objects. Each object will have the following structure:\n",
    "\n",
    "{\n",
    "    \"question\": \"A question generated from the chunk of text.\",\n",
    "}\n",
    "\n",
    "## Examples\n",
    "\n",
    "### Example 1\n",
    "\n",
    "#### Text:\n",
    "\n",
    "bitsandbytes enables accessible large language models via k-bit quantization for PyTorch. bitsandbytes provides three main features for dramatically reducing memory consumption for inference and training:\n",
    "\n",
    "- 8-bit optimizers uses block-wise quantization to maintain 32-bit performance at a small fraction of the memory cost.\n",
    "- LLM.int8() or 8-bit quantization enables large language model inference with only half the required memory and without any performance degradation. This method is based on vector-wise quantization to quantize most features to 8-bits and separately treating outliers with 16-bit matrix multiplication.\n",
    "- QLoRA or 4-bit quantization enables large language model training with several memory-saving techniques that don’t compromise performance. This method quantizes a model to 4-bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allow training.\n",
    "\n",
    "#### Output:\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"question\": \"What is the primary purpose of the bitsandbytes library?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the three main features bitsandbytes provides for reducing memory consumption?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does the LLM.int8() feature work to enable 8-bit inference without performance degradation?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is QLoRA and how does it utilize 4-bit quantization for training?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What quantization technique do the 8-bit optimizers in bitsandbytes use?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Why is vector-wise quantization used in the LLM.int8() method, and how are outliers handled?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "### Example 2\n",
    "\n",
    "#### Text:\n",
    "\n",
    "Creating your first star trail image\n",
    "How it works\n",
    "\n",
    "You’ve captured hundreds of photos, and now it’s time to blend them together to create your first star trail image. Each photo can be thought of as having two parts: the stars and the background.\n",
    "\n",
    "The background remains still, while the stars appear to move from frame to frame due to Earth’s rotation. Our goal is to keep the background consistent while revealing the stars’ motion across the sky.\n",
    "\n",
    "To do this, we gradually blend the images together to trace the path of each star. The pixels representing the background are mostly dark, while the ones representing the stars are bright.\n",
    "\n",
    "We use the lighten blending mode for this process, it takes the brighter value between a pixel in one image and the corresponding pixel in the next. This simple rule creates the illusion of continuous trails.\n",
    "\n",
    "If you can’t quite visualize it yet, don’t worry, I’ve included some illustrations to show exactly how this blending algorithm works.\n",
    "\n",
    "To demonstrate how the lighten blending mode works, I created five small 8×8 images. Each cell is colored either black (background) or white (stars).\n",
    "\n",
    "In the white cells, I’ve placed the number 1, which represents full brightness. In practice, each pixel in an 8-bit image stores a value between 0 and 255, where 255 corresponds to the maximum brightness. So, in this simplified example, 1 stands for 255.\n",
    "\n",
    "The black cells correspond to a brightness value of 0. For clarity, they are left empty in the diagram because they are greater in number than the white cells.\n",
    "\n",
    "In the first image, there are three stars. From one frame to the next, each star moves one pixel to the right and one pixel down. By the final frame, only one star remains visible, as the other two have moved outside the 8×8 grid.\n",
    "\n",
    "Now, let’s apply the lighten blending mode to the first two images. In the illustration, you’ll see them represented as inputs to the max() function. This function compares the pixel values from both images and keeps the brighter one for each position. The resulting image is a blend of the two, showing all the stars that were visible in either frame.\n",
    "\n",
    "The blended image becomes the new input for the next iteration of the max() function. The third image is then used as the second argument. Blend these two together, and repeat the process with the remaining images until you reach the fifth one.\n",
    "\n",
    "The final blended result is your complete star trail image, showing the continuous paths traced by the stars over time.\n",
    "\n",
    "#### Output:\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"question\": \"What is the primary goal when blending photos to create a star trail image?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Why do the stars appear to move from one photo to the next?\"\n",
    "    },\n",
    "        {\n",
    "        \"question\": \"What are the two main components of each image used to create a star trail photo?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What specific blending mode is used to create the star trail effect?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"In the simplified 8×8 grid example, what do the white cells and black cells represent in terms of pixel brightness?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What mathematical function is used in the illustration to represent the lighten blending mode?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does on a pixel level?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Describe the iterative process of blending the images to create the final star trail.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Don't generate questions that are similar to each other.\n",
    "- Questions should be relevant to the content of the chunk of text.\n",
    "- Generate only a JSON array as specified in the output format.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c3faf",
   "metadata": {},
   "source": [
    "### Create a Pydantic object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9679779b",
   "metadata": {},
   "source": [
    "We use a Pydantic object to define the structure of the output we want from the LLM. This will force the LLM to return the output in a structured format that we can easily parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc071454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62fbef4",
   "metadata": {},
   "source": [
    "### Use Qwen3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd836f",
   "metadata": {},
   "source": [
    "Create a few functions to generate questions using the Qwen3 model, create the user prompt, and parse the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcbb5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# Don't forget to start the llama.cpp server!\n",
    "LLAMA_SERVER_URL = \"http://localhost:36912\"\n",
    "\n",
    "\n",
    "def generate_questions(\n",
    "    system_prompt: str, user_prompt: str, max_tokens: int = 4096\n",
    ") -> list[Question]:\n",
    "    url = f\"{LLAMA_SERVER_URL}/v1/chat/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    data = {\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url=url, headers=headers, data=json.dumps(data))\n",
    "        response.raise_for_status()\n",
    "        response_json = response.json()\n",
    "\n",
    "        assistant_message = response_json[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        parsed_questions = parse_questions(assistant_message)\n",
    "        return parsed_questions\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_user_prompt(text_chunk: str) -> str:\n",
    "    return f\"\"\"Here is a chunk of text from a document:\n",
    "\n",
    "{text_chunk}\n",
    "\n",
    "Please generate relevant questions based on the content of this text chunk.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def parse_questions(raw_questions: str) -> list[Question]:\n",
    "    data = json.loads(raw_questions)\n",
    "    questions = [Question(**item) for item in data]\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b95bc6",
   "metadata": {},
   "source": [
    "Load the text chunks from the JSON file that you created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b072d363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"../data/chunks/rog_strix_gaming_notebook_pc_unscanned_file_chunks.json\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    text_chunks = json.load(f)\n",
    "\n",
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bcd051",
   "metadata": {},
   "source": [
    "Loop through each text chunk and generate questions using the function you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a124431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [10:21<00:00, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 400 Client Error: Bad Request for url: http://localhost:36912/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "question_answer_pairs = []\n",
    "for text_chunk_object in tqdm(iterable=text_chunks, total=len(text_chunks)):\n",
    "    text_chunk = text_chunk_object[\"text_chunk\"]\n",
    "    user_prompt = get_user_prompt(text_chunk)\n",
    "    questions = generate_questions(system_prompt=system_prompt, user_prompt=user_prompt)\n",
    "    if not questions:\n",
    "        continue\n",
    "\n",
    "    question_answer_pairs.extend(\n",
    "        [\n",
    "            {\"chunk_id\": text_chunk_object[\"id\"], \"question\": question_object.question}\n",
    "            for question_object in questions\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9454693e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_answer_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d44d496",
   "metadata": {},
   "source": [
    "### Save the question-answer pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d14c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "output_directory = \"../data/question_answer_pairs/\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "file_name = file_path.split(\"/\")[-1].replace(\".json\", \"_qa_pairs.json\")\n",
    "output_file_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "with open(output_file_path, \"w\") as f:\n",
    "    json.dump(question_answer_pairs, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark_embedding_models_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
